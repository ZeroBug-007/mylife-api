import feedparser
import json
import os
import re
from datetime import datetime

# è¿™é‡Œå¡«ä½  Zeabur ç”Ÿæˆçš„é‚£ä¸ª RSSHub åœ°å€
# æ ¼å¼ï¼šhttps://ä½ çš„åŸŸå.zeabur.app/twitter/user/ä½ çš„ç”¨æˆ·å
# å»ºè®®ï¼šä¸ºäº†å®‰å…¨ï¼Œè¿™ä¸ªåœ°å€æœ€å¥½é€šè¿‡ç¯å¢ƒå˜é‡ä¼ è¿›æ¥ï¼Œä¸‹é¢ä»£ç é‡Œæˆ‘ä¼šå†™ä¸¤ç§æ–¹å¼
RSS_URL = os.environ.get('RSS_URL')

def clean_html(raw_html):
    """
    å› ä¸º RSSHub è¿”å›çš„æ˜¯å¸¦ HTML æ ‡ç­¾çš„å†…å®¹ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ <br> æ¢æˆæ¢è¡Œï¼Œ
    æŠŠå…¶ä»–æ ‡ç­¾å»æ‰ï¼Œåªç•™çº¯æ–‡æœ¬ï¼Œæ–¹ä¾¿å‰ç«¯è‡ªå·±å†™æ ·å¼ã€‚
    """
    # 1. æŠŠ <br> æ¢æˆæ¢è¡Œç¬¦
    text = re.sub(r'<br\s*/*>', '\n', raw_html)
    # 2. å»æ‰æ‰€æœ‰ HTML æ ‡ç­¾ (<p>, <div>, <img> ç­‰)
    text = re.sub(r'<[^>]+>', '', text)
    return text.strip()

def fetch_tweets():
    if not RSS_URL:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ° RSS_URL ç¯å¢ƒå˜é‡ï¼Œè¯·æ£€æŸ¥ GitHub Actions è®¾ç½®ï¼")
        return

    print(f"ğŸ“¡ æ­£åœ¨è¿æ¥ RSS æº: {RSS_URL} ...")
    feed = feedparser.parse(RSS_URL)

    if feed.bozo:
        print("âš ï¸  è§£æ RSS æ—¶é‡åˆ°äº†ä¸€äº›å°é—®é¢˜ï¼Œä½†æˆ‘ä»¬å°†å°è¯•ç»§ç»­...")

    if len(feed.entries) == 0:
        print("ğŸ“­  æ²¡æœ‰è·å–åˆ°æ¨æ–‡ï¼Œå¯èƒ½æ˜¯ Cookie è¿‡æœŸäº†ï¼Œæˆ–è€…ä½ æœ€è¿‘æ²¡å‘æ¨ã€‚")
        return

    tweets_data = []

    for entry in feed.entries:
        # æå–å›¾ç‰‡ï¼šRSSHub é€šå¸¸æŠŠå›¾ç‰‡æ”¾åœ¨ description é‡Œçš„ <img src="...">
        # æˆ–è€… enclosure å±æ€§é‡Œï¼Œè¿™é‡Œåšä¸€ä¸ªç®€å•çš„æå–é€»è¾‘
        images = []

        # å°è¯•ä» description ä¸­æå–å›¾ç‰‡é“¾æ¥
        img_urls = re.findall(r'<img src="([^"]+)"', entry.description)
        # RSSHub çš„å›¾ç‰‡é€šå¸¸æ˜¯ https://pbs.twimg.com/...
        # ä¸ºäº†é˜²ç›—é“¾ï¼Œæœ‰æ—¶ä½ å‰ç«¯å¯èƒ½éœ€è¦ç”¨ images.weserv.nl è¿™ç§æœåŠ¡ä»£ç†ä¸€ä¸‹ï¼Œæˆ–è€…ç›´æ¥ç”¨
        images.extend(img_urls)

        # æ¸…æ´—æ­£æ–‡
        content_text = clean_html(entry.description)

        # æ ¼å¼åŒ–æ—¶é—´ (RSS çš„æ—¶é—´æ ¼å¼æ¯”è¾ƒä¹±ï¼Œè¿™é‡Œè½¬æˆæ ‡å‡†çš„ YYYY-MM-DD HH:mm)
        try:
            # entry.published_parsed æ˜¯ä¸€ä¸ªæ—¶é—´å…ƒç»„
            dt = datetime(*entry.published_parsed[:6])
            date_str = dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            date_str = entry.published

        tweet = {
            "id": entry.guid,  # æ¨æ–‡å”¯ä¸€ID
            "link": entry.link, # æ¨æ–‡åŸé“¾æ¥
            "date": date_str,   # å‘å¸ƒæ—¶é—´
            "content": content_text, # çº¯æ–‡æœ¬å†…å®¹
            "images": images    # å›¾ç‰‡åˆ—è¡¨
        }
        tweets_data.append(tweet)

    # ä¿å­˜åˆ°æ–‡ä»¶
    # å‡è®¾æˆ‘ä»¬è¦å­˜åˆ° public/data/tweets.json (æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´)
    # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»º
    output_dir = 'public/data'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_path = os.path.join(output_dir, 'tweets.json')

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(tweets_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… æˆåŠŸæŠ“å–äº† {len(tweets_data)} æ¡æ¨æ–‡ï¼Œå·²ä¿å­˜åˆ° {output_path}")

if __name__ == "__main__":
    fetch_tweets()
